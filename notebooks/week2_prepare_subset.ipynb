{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65665018-1d1d-4e7b-8da7-aec8a6377f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking folder: C:\\BLAST\\variant-effect-prediction\\notebooks\\data\n",
      "Found 11 files:\n",
      " - .ipynb_checkpoints\n",
      " - Unconfirmed 243949.crdownload\n",
      " - clinvar.vcf\n",
      " - clinvar.vcf.gz\n",
      " - clinvar.vcf.gz.tbi\n",
      " - clinvar_features.csv\n",
      " - clinvar_labeled.csv\n",
      " - clinvar_ml_ready.csv\n",
      " - data\n",
      " - variant_summary.txt\n",
      " - variant_summary.txt.gz\n",
      "\n",
      "Possible ClinVar/variant-like files:\n",
      " * clinvar.vcf\n",
      " * clinvar.vcf.gz\n",
      " * clinvar.vcf.gz.tbi\n",
      " * clinvar_features.csv\n",
      " * clinvar_labeled.csv\n",
      " * clinvar_ml_ready.csv\n",
      " * variant_summary.txt\n",
      " * variant_summary.txt.gz\n"
     ]
    }
   ],
   "source": [
    "# Cell A: list files in your notebooks\\data folder\n",
    "import os\n",
    "project_dir = r\"C:\\BLAST\\variant-effect-prediction\"\n",
    "alt_data_dir = os.path.join(project_dir, \"notebooks\", \"data\")\n",
    "\n",
    "print(\"Checking folder:\", alt_data_dir)\n",
    "if not os.path.exists(alt_data_dir):\n",
    "    raise FileNotFoundError(f\"Folder not found: {alt_data_dir}\")\n",
    "\n",
    "files = sorted(os.listdir(alt_data_dir))\n",
    "print(f\"Found {len(files)} files:\")\n",
    "for f in files:\n",
    "    print(\" -\", f)\n",
    "    \n",
    "# show if any file name looks like ClinVar/variant\n",
    "candidates = [f for f in files if any(k in f.lower() for k in [\"variant_summary\",\"clinvar\",\"variant\",\"clinvar_variant\",\"clinvar_summary\", \".vcf\", \".txt\", \".csv\", \".gz\"])]\n",
    "print(\"\\nPossible ClinVar/variant-like files:\")\n",
    "for f in candidates:\n",
    "    print(\" *\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaaae59-0354-4a71-b954-4a68eccb5ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT DIR: C:\\BLAST\\variant-effect-prediction\n",
      "ALT DATA DIR: C:\\BLAST\\variant-effect-prediction\\notebooks\\data\n",
      "RAW DIR: C:\\BLAST\\variant-effect-prediction\\data\\raw\n",
      "PROCESSED DIR: C:\\BLAST\\variant-effect-prediction\\data\\processed\n",
      "\n",
      "--- Listing ALT_DATA_DIR ---\n",
      "11 files found in C:\\BLAST\\variant-effect-prediction\\notebooks\\data:\n",
      " - .ipynb_checkpoints   (0 bytes)\n",
      " - Unconfirmed 243949.crdownload   (395037542 bytes)\n",
      " - clinvar.vcf   (0 bytes)\n",
      " - clinvar.vcf.gz   (174846686 bytes)\n",
      " - clinvar.vcf.gz.tbi   (571720 bytes)\n",
      " - clinvar_features.csv   (69043392 bytes)\n",
      " - clinvar_labeled.csv   (373504980 bytes)\n",
      " - clinvar_ml_ready.csv   (126205240 bytes)\n",
      " - data   (0 bytes)\n",
      " - variant_summary.txt   (0 bytes)\n",
      " - variant_summary.txt.gz   (395037542 bytes)\n",
      "\n",
      "--- Listing RAW_DIR ---\n",
      "2 files in C:\\BLAST\\variant-effect-prediction\\data\\raw: ['variant_summary.txt', 'variant_summary.txt.gz']\n",
      "\n",
      "--- Listing PROCESSED_DIR ---\n",
      "0 files in C:\\BLAST\\variant-effect-prediction\\data\\processed: []\n",
      "\n",
      "Auto-detected candidate: variant_summary.txt.gz\n",
      "Source path: C:\\BLAST\\variant-effect-prediction\\notebooks\\data\\variant_summary.txt.gz\n",
      "File already exists in raw dir: C:\\BLAST\\variant-effect-prediction\\data\\raw\\variant_summary.txt.gz\n",
      "\n",
      "Attempting to load the candidate file (this may take some seconds)...\n",
      " -> reading gzipped tab-delimited via pandas\n"
     ]
    }
   ],
   "source": [
    "# Debug + robust runner for week2_prepare_subset\n",
    "import os, shutil, gzip, sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "project_dir = r\"C:\\BLAST\\variant-effect-prediction\"\n",
    "alt_data_dir = os.path.join(project_dir, \"notebooks\", \"data\")\n",
    "raw_dir = os.path.join(project_dir, \"data\", \"raw\")\n",
    "proc_dir = os.path.join(project_dir, \"data\", \"processed\")\n",
    "\n",
    "print(\"PROJECT DIR:\", project_dir)\n",
    "print(\"ALT DATA DIR:\", alt_data_dir)\n",
    "print(\"RAW DIR:\", raw_dir)\n",
    "print(\"PROCESSED DIR:\", proc_dir)\n",
    "\n",
    "# ensure canonical folders exist\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(proc_dir, exist_ok=True)\n",
    "\n",
    "# 1) list alt data dir contents (the place you said the file is)\n",
    "print(\"\\n--- Listing ALT_DATA_DIR ---\")\n",
    "if os.path.exists(alt_data_dir):\n",
    "    alt_files = sorted(os.listdir(alt_data_dir))\n",
    "    print(f\"{len(alt_files)} files found in {alt_data_dir}:\")\n",
    "    for f in alt_files:\n",
    "        p = os.path.join(alt_data_dir, f)\n",
    "        size = os.path.getsize(p)\n",
    "        print(f\" - {f}   ({size} bytes)\")\n",
    "else:\n",
    "    print(f\"ALT data dir does NOT exist: {alt_data_dir}\")\n",
    "    alt_files = []\n",
    "\n",
    "# 2) list canonical raw dir contents\n",
    "print(\"\\n--- Listing RAW_DIR ---\")\n",
    "raw_files = sorted(os.listdir(raw_dir)) if os.path.exists(raw_dir) else []\n",
    "print(f\"{len(raw_files)} files in {raw_dir}: {raw_files}\")\n",
    "\n",
    "# 3) list processed dir contents\n",
    "print(\"\\n--- Listing PROCESSED_DIR ---\")\n",
    "proc_files = sorted(os.listdir(proc_dir)) if os.path.exists(proc_dir) else []\n",
    "print(f\"{len(proc_files)} files in {proc_dir}: {proc_files}\")\n",
    "\n",
    "# 4) try to auto-detect a clinvar-like file in alt_data_dir\n",
    "priority = [\"variant_summary.txt.gz\", \"variant_summary.txt\", \"clinvar.vcf.gz\", \"clinvar.vcf\",\n",
    "            \"clinvar_features.csv\", \"clinvar_labeled.csv\", \"clinvar_ml_ready.csv\"]\n",
    "chosen = None\n",
    "\n",
    "# pick exact priority match first\n",
    "for name in priority:\n",
    "    if name in alt_files:\n",
    "        chosen = name\n",
    "        break\n",
    "\n",
    "# else, pick first file that looks appropriate\n",
    "if chosen is None:\n",
    "    for f in alt_files:\n",
    "        if any(k in f.lower() for k in [\"variant_summary\",\"clinvar\",\"variant\",\".vcf\",\".txt\",\".csv\",\".gz\"]):\n",
    "            chosen = f\n",
    "            break\n",
    "\n",
    "print(\"\\nAuto-detected candidate:\", chosen)\n",
    "\n",
    "if chosen is None:\n",
    "    print(\"\\nNo candidate found in alt_data_dir. Please check where you downloaded the ClinVar file.\")\n",
    "    print(\"If you downloaded via browser, it may be in Downloads. Move the file into:\")\n",
    "    print(\"  \", alt_data_dir)\n",
    "else:\n",
    "    src_path = os.path.join(alt_data_dir, chosen)\n",
    "    print(\"Source path:\", src_path)\n",
    "    # 5) attempt to copy to raw_dir (so code uses canonical raw path)\n",
    "    dst_path = os.path.join(raw_dir, chosen)\n",
    "    try:\n",
    "        if not os.path.exists(dst_path):\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            print(\"Copied to raw dir:\", dst_path)\n",
    "        else:\n",
    "            print(\"File already exists in raw dir:\", dst_path)\n",
    "    except Exception as e:\n",
    "        print(\"Could not copy file to raw dir. Error:\")\n",
    "        traceback.print_exc()\n",
    "        dst_path = src_path\n",
    "        print(\"Will attempt to read from source path:\", dst_path)\n",
    "\n",
    "    # 6) try to load the file robustly and create subset\n",
    "    def load_clinvar_table(path):\n",
    "        low = path.lower()\n",
    "        try:\n",
    "            if low.endswith(\".gz\"):\n",
    "                print(\" -> reading gzipped tab-delimited via pandas\")\n",
    "                return pd.read_csv(path, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n",
    "            if low.endswith(\".vcf\") or low.endswith(\".vcf.gz\"):\n",
    "                print(\" -> attempting to read VCF-like file with pandas (skip meta lines)\")\n",
    "                # pandas can read vcf rows if we skip '#'\n",
    "                if low.endswith(\".vcf.gz\"):\n",
    "                    import gzip\n",
    "                    with gzip.open(path, \"rt\", errors=\"ignore\") as gf:\n",
    "                        for line in gf:\n",
    "                            if line.startswith(\"#CHROM\"):\n",
    "                                header = line.strip().lstrip(\"#\")\n",
    "                                break\n",
    "                    if 'header' in locals():\n",
    "                        cols = header.split(\"\\t\")\n",
    "                        return pd.read_csv(path, sep=\"\\t\", comment=\"#\", names=cols, low_memory=False)\n",
    "                    else:\n",
    "                        return pd.read_csv(path, sep=\"\\t\", comment=\"#\", low_memory=False)\n",
    "                else:\n",
    "                    return pd.read_csv(path, sep=\"\\t\", comment=\"#\", low_memory=False)\n",
    "            if low.endswith(\".txt\") or low.endswith(\".tsv\"):\n",
    "                print(\" -> reading plain TSV\")\n",
    "                return pd.read_csv(path, sep=\"\\t\", low_memory=False)\n",
    "            if low.endswith(\".csv\"):\n",
    "                print(\" -> reading CSV\")\n",
    "                return pd.read_csv(path, low_memory=False)\n",
    "            # fallback attempts\n",
    "            try:\n",
    "                print(\" -> fallback: try gzip read\")\n",
    "                return pd.read_csv(path, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n",
    "            except Exception:\n",
    "                print(\" -> fallback: try plain tab-delimited read\")\n",
    "                return pd.read_csv(path, sep=\"\\t\", low_memory=False)\n",
    "        except Exception:\n",
    "            print(\"Error when attempting to load file:\", path)\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    try:\n",
    "        print(\"\\nAttempting to load the candidate file (this may take some seconds)...\")\n",
    "        df = load_clinvar_table(dst_path)\n",
    "        print(\"Loaded dataframe shape:\", df.shape)\n",
    "        print(\"First columns (up to 40):\", df.columns.tolist()[:40])\n",
    "\n",
    "        # normalize col names to lowercase\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        df = df.rename(columns={c:c.lower() for c in df.columns})\n",
    "\n",
    "        # try to find clinical significance column\n",
    "        cln_candidates = [c for c in df.columns if \"clinsig\" in c or \"clinical\" in c or \"significance\" in c]\n",
    "        print(\"Possible clinical significance columns:\", cln_candidates[:10])\n",
    "        if not cln_candidates:\n",
    "            print(\"No clinical significance-like column found. Here are the column names:\")\n",
    "            print(df.columns.tolist())\n",
    "            raise ValueError(\"Cannot detect clinical significance column. Inspect columns above.\")\n",
    "        cln_col = cln_candidates[0]\n",
    "        print(\"Using clinical significance column:\", cln_col)\n",
    "\n",
    "        # create simple CLNSIG\n",
    "        df[\"clnsig_simple\"] = df[cln_col].astype(str).str.split(\";\").str[0].str.strip()\n",
    "        mask = df[\"clnsig_simple\"].str.lower().isin([\"pathogenic\",\"benign\"])\n",
    "        df_filtered = df[mask].copy()\n",
    "        print(\"Filtered to Pathogenic/Benign rows:\", len(df_filtered))\n",
    "\n",
    "        # try SNV filter via ref/alt-like columns\n",
    "        ref_col = next((c for c in df_filtered.columns if c in [\"ref\",\"referenceallele\",\"reference_allele\"]), None)\n",
    "        alt_col = next((c for c in df_filtered.columns if c in [\"alt\",\"alternateallele\",\"alternate_allele\"]), None)\n",
    "        print(\"Detected REF col:\", ref_col, \"ALT col:\", alt_col)\n",
    "        if ref_col and alt_col:\n",
    "            before = len(df_filtered)\n",
    "            df_filtered = df_filtered[df_filtered[ref_col].astype(str).str.len()==1]\n",
    "            df_filtered = df_filtered[df_filtered[alt_col].astype(str).str.len()==1]\n",
    "            print(f\"After simple SNV filter: {before} -> {len(df_filtered)}\")\n",
    "        else:\n",
    "            print(\"Skipping SNV length filter (REF/ALT not both found).\")\n",
    "\n",
    "        # select preferred columns\n",
    "        preferred = [\"alleleid\",\"genesymbol\",\"gene\",\"clnsig\",\"clnsig_simple\",\"clinicalsignificance\",\"type\",\"name\",\"chr\",\"chrom\",\"start\",\"position\",\"ref\",\"alt\",\"referenceallele\",\"alternateallele\",\"rsid\",\"variationid\",\"reviewstatus\"]\n",
    "        keep = [c for c in preferred if c in df_filtered.columns]\n",
    "        if not keep:\n",
    "            keep = df_filtered.columns.tolist()[:12]\n",
    "            print(\"Preferred columns not found; defaulting to first 12 columns:\", keep)\n",
    "        else:\n",
    "            print(\"Keeping columns:\", keep)\n",
    "\n",
    "        subset = df_filtered[keep].reset_index(drop=True)\n",
    "\n",
    "        n = 10000\n",
    "        if len(subset) > n:\n",
    "            subset = subset.sample(n, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Sampled down to {n}\")\n",
    "\n",
    "        out_name = f\"clinvar_subset_{len(subset)}.csv\"\n",
    "        out_path = os.path.join(proc_dir, out_name)\n",
    "        subset.to_csv(out_path, index=False)\n",
    "        print(\"Saved processed subset to:\", out_path)\n",
    "\n",
    "        print(\"\\nPreview:\")\n",
    "        display(subset.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nFAILED to create processed subset. Error details:\")\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nPlease paste the entire output of this cell here so I can diagnose.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c834c-daba-4b20-94f5-55b5913fd42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunked processing: extract Pathogenic / Benign rows and create a balanced subset\n",
    "import os, gzip, shutil, math\n",
    "import pandas as pd\n",
    "\n",
    "project_dir = r\"C:\\BLAST\\variant-effect-prediction\"\n",
    "alt_data_dir = os.path.join(project_dir, \"notebooks\", \"data\")\n",
    "raw_dir = os.path.join(project_dir, \"data\", \"raw\")\n",
    "proc_dir = os.path.join(project_dir, \"data\", \"processed\")\n",
    "os.makedirs(proc_dir, exist_ok=True)\n",
    "\n",
    "gz_path = os.path.join(raw_dir, \"variant_summary.txt.gz\")\n",
    "if not os.path.exists(gz_path):\n",
    "    # fallback to alt location\n",
    "    alt_candidate = os.path.join(alt_data_dir, \"variant_summary.txt.gz\")\n",
    "    if os.path.exists(alt_candidate):\n",
    "        gz_path = alt_candidate\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find variant_summary.txt.gz in {raw_dir} or {alt_data_dir}.\")\n",
    "\n",
    "print(\"Reading from:\", gz_path)\n",
    "chunksize = 100_000  # adjust if your memory is limited\n",
    "temp_out = os.path.join(proc_dir, \"clinvar_filtered_temp.csv\")\n",
    "\n",
    "# remove temp if exists\n",
    "if os.path.exists(temp_out):\n",
    "    os.remove(temp_out)\n",
    "\n",
    "# We will auto-detect the clinical significance column from first chunk\n",
    "first_chunk = True\n",
    "cln_col = None\n",
    "rows_written = 0\n",
    "total_candidates = 0\n",
    "\n",
    "for chunk_i, chunk in enumerate(pd.read_csv(gz_path, sep=\"\\t\", compression=\"gzip\", chunksize=chunksize, low_memory=False)):\n",
    "    if first_chunk:\n",
    "        # normalize column names to simple lowercase tokens for detection\n",
    "        cols_lower = [str(c).strip().lower() for c in chunk.columns]\n",
    "        # find a good clinical significance candidate\n",
    "        for c, orig in zip(cols_lower, chunk.columns):\n",
    "            if (\"clinsig\" in c) or (\"clinical\" in c and \"sign\" in c) or (\"significance\" in c):\n",
    "                cln_col = orig\n",
    "                break\n",
    "        if cln_col is None:\n",
    "            # fallback: print columns and raise error\n",
    "            print(\"Could not detect clinical significance column from first chunk. Columns were:\")\n",
    "            print(chunk.columns.tolist())\n",
    "            raise ValueError(\"No clinical significance column detected.\")\n",
    "        print(\"Detected clinical-significance column:\", cln_col)\n",
    "        first_chunk = False\n",
    "\n",
    "    # create normalized clnsig_simple\n",
    "    chunk[\"clnsig_simple\"] = chunk[cln_col].astype(str).str.split(\";\").str[0].str.strip()\n",
    "    mask = chunk[\"clnsig_simple\"].str.lower().isin([\"pathogenic\", \"benign\"])\n",
    "    df_sel = chunk.loc[mask].copy()\n",
    "    total_candidates += len(df_sel)\n",
    "\n",
    "    if len(df_sel) == 0:\n",
    "        if (chunk_i % 5) == 0:\n",
    "            print(f\"Chunk {chunk_i}: no matching rows\")\n",
    "        continue\n",
    "\n",
    "    # choose columns to keep (prefer this list)\n",
    "    preferred = [\"alleleid\",\"genesymbol\",\"gene\",\"clnsig\",\"clnsig_simple\",\"clinicalsignificance\",\"type\",\"name\",\n",
    "                 \"chr\",\"chrom\",\"start\",\"position\",\"ref\",\"alt\",\"referenceallele\",\"alternateallele\",\"rsid\",\"variationid\",\"reviewstatus\"]\n",
    "    keep = [c for c in preferred if c in df_sel.columns]\n",
    "    if not keep:\n",
    "        keep = df_sel.columns.tolist()  # keep all if none matched\n",
    "    df_out = df_sel[keep]\n",
    "\n",
    "    # append to temp CSV\n",
    "    header = not os.path.exists(temp_out)\n",
    "    df_out.to_csv(temp_out, mode=\"a\", index=False, header=header)\n",
    "    rows_written += len(df_out)\n",
    "\n",
    "    if (chunk_i % 5) == 0:\n",
    "        print(f\"Processed chunk {chunk_i}, matched rows added: {len(df_out)}, total so far: {rows_written}\")\n",
    "\n",
    "print(f\"\\nFinished chunked scan. Total matching rows found: {rows_written} (saved to temp: {temp_out})\")\n",
    "\n",
    "if rows_written == 0:\n",
    "    raise ValueError(\"No Pathogenic/Benign rows found. Check clinical significance column and file contents.\")\n",
    "\n",
    "# Now load the temp file (it should be much smaller) and create a balanced sample up to N_total\n",
    "df_all = pd.read_csv(temp_out, low_memory=False)\n",
    "print(\"Filtered table shape:\", df_all.shape)\n",
    "print(\"Label counts (raw):\")\n",
    "print(df_all[\"clnsig_simple\"].value_counts())\n",
    "\n",
    "# target total rows\n",
    "N_total = 10000\n",
    "# compute per-class target aiming for balance\n",
    "n_path = min(len(df_all[df_all[\"clnsig_simple\"].str.lower()==\"pathogenic\"]), N_total//2)\n",
    "n_ben  = min(len(df_all[df_all[\"clnsig_simple\"].str.lower()==\"benign\"]), N_total - n_path)\n",
    "# if insufficient in one class, fill with more from the other\n",
    "remaining = N_total - (n_path + n_ben)\n",
    "if remaining > 0:\n",
    "    # add from whichever class has more remaining\n",
    "    more_class = \"pathogenic\" if len(df_all[df_all[\"clnsig_simple\"].str.lower()==\"pathogenic\"]) > len(df_all[df_all[\"clnsig_simple\"].str.lower()==\"benign\"]) else \"benign\"\n",
    "    more_avail = len(df_all[df_all[\"clnsig_simple\"].str.lower()==more_class]) - (n_path if more_class==\"pathogenic\" else n_ben)\n",
    "    add = min(remaining, max(0, more_avail))\n",
    "    if add > 0:\n",
    "        if more_class==\"pathogenic\":\n",
    "            n_path += add\n",
    "        else:\n",
    "            n_ben += add\n",
    "\n",
    "print(f\"Sampling targets -> Pathogenic: {n_path}, Benign: {n_ben} (total target {n_path+n_ben})\")\n",
    "\n",
    "df_path = df_all[df_all[\"clnsig_simple\"].str.lower()==\"pathogenic\"]\n",
    "df_ben  = df_all[df_all[\"clnsig_simple\"].str.lower()==\"benign\"]\n",
    "\n",
    "# sample without replacement (if available), else take all\n",
    "df_path_s = df_path.sample(n=n_path, random_state=42) if len(df_path) > n_path else df_path\n",
    "df_ben_s  = df_ben.sample(n=n_ben, random_state=42) if len(df_ben) > n_ben else df_ben\n",
    "\n",
    "subset = pd.concat([df_path_s, df_ben_s]).reset_index(drop=True)\n",
    "print(\"Final subset shape:\", subset.shape)\n",
    "out_name = f\"clinvar_subset_{len(subset)}.csv\"\n",
    "out_path = os.path.join(proc_dir, out_name)\n",
    "subset.to_csv(out_path, index=False)\n",
    "print(\"Saved final subset to:\", out_path)\n",
    "\n",
    "# cleanup temp if you want (uncomment to remove)\n",
    "# os.remove(temp_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e757f87-e16e-464a-84c7-28803ebc9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "project_dir = r\"C:\\BLAST\\variant-effect-prediction\"\n",
    "raw_dir = os.path.join(project_dir, \"data\", \"raw\")\n",
    "proc_dir = os.path.join(project_dir, \"data\", \"processed\")\n",
    "os.makedirs(proc_dir, exist_ok=True)\n",
    "\n",
    "raw_path = os.path.join(raw_dir, \"variant_summary.txt.gz\")\n",
    "print(\"Reading from:\", raw_path)\n",
    "\n",
    "chunksize = 500_000\n",
    "matched_rows = []\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(raw_path, sep=\"\\t\", compression=\"gzip\", low_memory=False, chunksize=chunksize)):\n",
    "    if i == 0:\n",
    "        print(\"Columns:\", list(chunk.columns)[:20])\n",
    "    # detect clin sig column\n",
    "    cln_candidates = [c for c in chunk.columns if \"significance\" in c.lower() or \"clinsig\" in c.lower()]\n",
    "    if not cln_candidates:\n",
    "        continue\n",
    "    cln_col = cln_candidates[0]\n",
    "    # keep benign / pathogenic\n",
    "    sub = chunk[chunk[cln_col].astype(str).str.lower().str.contains(\"benign|pathogenic\", na=False)].copy()\n",
    "    if not sub.empty:\n",
    "        matched_rows.append(sub)\n",
    "    print(f\"Chunk {i}: matched {len(sub)}\")\n",
    "\n",
    "df = pd.concat(matched_rows, ignore_index=True)\n",
    "print(\"Total matched:\", len(df))\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = [c.lower().strip() for c in df.columns]\n",
    "\n",
    "# Simplify clin sig\n",
    "cln_col = [c for c in df.columns if \"significance\" in c or \"clinsig\" in c][0]\n",
    "df[\"clnsig_simple\"] = df[cln_col].astype(str).str.split(\";\").str[0].str.strip()\n",
    "\n",
    "# Filter for simple benign/pathogenic\n",
    "mask = df[\"clnsig_simple\"].str.lower().isin([\"benign\", \"pathogenic\"])\n",
    "df_filtered = df[mask].copy()\n",
    "print(\"Filtered:\", len(df_filtered))\n",
    "\n",
    "# Pick essential columns\n",
    "preferred = [\"alleleid\",\"genesymbol\",\"gene\",\"clnsig_simple\",\"clinicalsignificance\",\n",
    "             \"name\",\"ref\",\"alt\",\"rsid\",\"variationid\",\"reviewstatus\"]\n",
    "keep = [c for c in preferred if c in df_filtered.columns]\n",
    "subset = df_filtered[keep].reset_index(drop=True)\n",
    "\n",
    "# Downsample\n",
    "N_total = 10_000  # adjust here (e.g., 50000 if you want even more)\n",
    "if len(subset) > N_total:\n",
    "    subset = subset.sample(N_total, random_state=42)\n",
    "print(\"Subset shape:\", subset.shape)\n",
    "\n",
    "out_path = os.path.join(proc_dir, f\"clinvar_subset_{len(subset)}.csv\")\n",
    "subset.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "display(subset.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa585a14-cb1c-431e-8e8d-0d25ec61cd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
